---
title: "BDA Project"
author: "|AUTHORS|"
date: "|DATE|"
output: 
  pdf_document:
    extra_dependencies: ["multirow", "float"]
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: 72
---

\renewcommand{\arraystretch}{1.2}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
```

```{r, master, include=FALSE, eval=FALSE}
# libraries
library(rstan)
library(loo)
library(latex2exp)

# read
data <- read.csv("data/preprocessed_data.csv")

# variables

# pool variables
x1 <- data[,'Continent']
x2 <- data[, 'Fullscale_IQ_Change']
x3 <- data[,'Schooling_Index']
x4 <- data[,'Decade']
xpred <- 11
latest_data <- data[data['Decade'] == 10]
n <- length(latest_data)
schooling <- as.numeric(unlist(latest_data[16:n]))
mu <- mean(schooling)
mu_vec <- c(mu * 1.05, mu * 1.1, mu * 1.15)


# sep variables
data1 <- data[data$Continent == "Europe", ]
data2 <- data[data$Continent == "Oceania", ]
data3 <- data[data$Continent == "Africa", ]
data4 <- data[data$Continent == "Asia", ]
data5 <- data[data$Continent == "America", ]


# Data objects for Stan
pool_data_temp <- list(N = length(x3),
           x1 = x4,
           y =  x2,
           xpred = xpred)

pool_data_schooling <- list(N = length(x3),
           x1 = x3,
           y = x2)

pool_data_comb <- list(N = length(x3),
           x1 = x4,
           x2 = x3,
           y = x2,
           xpred_decade = xpred,
           xpred_schooling = mu_vec
)


sep_data_temp <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),

  x1 = data1[, "Decade"],
  x2 = data2[, "Decade"],
  x3 = data3[, "Decade"],
  x4 = data4[, "Decade"],
  x5 = data5[, "Decade"],
  
  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"]
)

sep_data_schooling <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),

  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"],
  
  z1 = data1[, "Schooling_Index"],
  z2 = data2[, "Schooling_Index"],
  z3 = data3[, "Schooling_Index"],
  z4 = data4[, "Schooling_Index"],
  z5 = data5[, "Schooling_Index"]
)

sep_data_comb <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),

  x1 = data1[, "Decade"],
  x2 = data2[, "Decade"],
  x3 = data3[, "Decade"],
  x4 = data4[, "Decade"],
  x5 = data5[, "Decade"],
  
  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"],
  
  z1 = data1[, "Schooling_Index"],
  z2 = data2[, "Schooling_Index"],
  z3 = data3[, "Schooling_Index"],
  z4 = data4[, "Schooling_Index"],
  z5 = data5[, "Schooling_Index"]
)

hier_data_temp <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),
  x1 = data1[, "Decade"],
  x2 = data2[, "Decade"],
  x3 = data3[, "Decade"],
  x4 = data4[, "Decade"],
  x5 = data5[, "Decade"],
  
  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"]
)

hier_data_schooling <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),
  
  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"],
  
  z1 = data1[, "Schooling_Index"],
  z2 = data2[, "Schooling_Index"],
  z3 = data3[, "Schooling_Index"],
  z4 = data4[, "Schooling_Index"],
  z5 = data5[, "Schooling_Index"]
)

hier_data_comb <- list(
  C = 5,
  N1 = length(data1$Decade),
  N2 = length(data2$Decade),
  N3 = length(data3$Decade),
  N4 = length(data4$Decade),
  N5 = length(data5$Decade),
  x1 = data1[, "Decade"],
  x2 = data2[, "Decade"],
  x3 = data3[, "Decade"],
  x4 = data4[, "Decade"],
  x5 = data5[, "Decade"],
  
  y1 = data1[, "Fullscale_IQ_Change"],
  y2 = data2[, "Fullscale_IQ_Change"],
  y3 = data3[, "Fullscale_IQ_Change"],
  y4 = data4[, "Fullscale_IQ_Change"],
  y5 = data5[, "Fullscale_IQ_Change"],
  
  z1 = data1[, "Schooling_Index"],
  z2 = data2[, "Schooling_Index"],
  z3 = data3[, "Schooling_Index"],
  z4 = data4[, "Schooling_Index"],
  z5 = data5[, "Schooling_Index"]
)
  
#  Stan Models

# Separate Models
fit_sep_temp <- stan(file = 'models/separate/sep_temporal.stan',
                     data = sep_data_temp, 
                     seed=1)

fit_sep_school <- stan(file = 'models/separate/sep_schooling.stan',
                       data = sep_data_schooling, 
                       seed=1)

fit_sep_comb<- stan(file = 'models/separate/sep_joint.stan',
                       data = sep_data_comb, 
                       seed=1)

# Pooled Model
fit_pool_temp <- stan(file = 'models/pooled/pool.stan', 
                      data = pool_data_temp, 
                      seed= 1)

fit_pool_school <- stan(file = 'models/pooled/pool_schooling.stan',
                        data = pool_data_schooling, 
                        seed = 1)

fit_pool_comb <- stan(file = 'models/pooled/pool_adv.stan', 
                      data = pool_data_comb, 
                      seed = 1)
# Hierarchical Mode
fit_hier_temp <- stan(file='models/hierarchical/hier.stan', 
                      data=hier_data_temp, 
                      seed=1)

fit_hier_school <-stan(file='models/hierarchical/hier_school.stan', 
                       data = hier_data_schooling, 
                       seed=1)

fit_hier_comb <- stan(file='models/hierarchical/hier_adv.stan', 
                      data=hier_data_comb, 
                      seed=1)
```

# Introduction

IQ is a psychometric developed in 1904 by French Psychologsts Alfred Binet and Theodore Simon
as a measurement intended to measure the test taker's cognitive ability. 
IQ is by convention normally distributed with a mean of 100 points and a standard deviation of 15
points. Full-scale IQ is an IQ score derived by averaging over the
results of a test battery of several subtests which are intended to
measure different intelligence domains. IQ tests are not practiced for
and as such are supposed to measure some inherent feature of the test
taker. IQ as a metric has been widely criticized, due to its spurious
relation to an ordinary language understanding of intelligence and
historically grave misuse of results, but remains in wide use. The
observed global trend of the average IQ to rise over generations has
been termed the Flynn effect. [@IQ_paper]. This report does not assume
any position on the validity of IQ as a metric.

In this report we will attempt to replicate the Flynn effect with our
data and to explore the dependency between IQ change and change in
average years of schooling in the population [@AHDI_paper]. We will
explore these dependencies both globally and per continent. The trend in
IQ will be validated with a model that explores temporal dependency
alone and a further model that factors in schooling level. We will model
the data with a pooled, separate and hierarchical model. The models will
explore the relationship between the passage of time and the development
of IQ scores from the start of testing and the relationship between
schooling level and the development of IQ scores from the start of
testing.

-   showing some illustrative figure **just plot the data as different
    colored points per regions?** **regression lines for each region on
    the same graph?**
    
The raw data can be visualized as follows with a scatterplot of cumulative Full-scale
IQ change with respect to time, a histogram of the distribution of this IQ change per continent, and
histograms to show the distribution of schooling level:
```{r, data_viz, fig.show='hold', echo=FALSE}
# scatterplot of decade, IQ for different colors for continents
# data1 <- data[data$Continent == "Europe", ] red
# data2 <- data[data$Continent == "Oceania", ] green
# data3 <- data[data$Continent == "Africa", ] blue 
# data4 <- data[data$Continent == "Asia", ] magenta
# data5 <- data[data$Continent == "America", ] orange
plot(x=data5[,"Decade"], y=data5[,"Fullscale_IQ_Change"], col="orange",
     ylab=TeX("cumulative $\\Delta IQ$"),
     xlab="number of decades from the beginning of measurement")
points(data1[,"Decade"], data1[,"Fullscale_IQ_Change"], col="red")
points(data2[,"Decade"], data2[,"Fullscale_IQ_Change"], col="green")
points(data3[,"Decade"], data3[,"Fullscale_IQ_Change"], col="blue")
points(data4[,"Decade"], data4[,"Fullscale_IQ_Change"], col="magenta")
legend(x="topleft", legend=c("America", "Europe", "Oceania", "Africa", "Asia"),
       col=c("orange", "red", "green", "blue", "magenta"), lty=1, cex=0.8)
```

```{r, data_viz_IQ_hist, fig.show="hold", echo=FALSE}
# delta IQ histogram for continent
hist(data5[,"Fullscale_IQ_Change"], col="orange",  border=F, freq=F, xlab=TeX("$\\Delta IQ"), main=TeX("$\\Delta IQ$ histogram, America"))
hist(data1[,"Fullscale_IQ_Change"], col="red", border=F, freq=F, xlab=TeX("$\\Delta IQ"), main=TeX("$\\Delta IQ$ histogram, Europe"))
hist(data2[,"Fullscale_IQ_Change"], col="green", border=F, freq=F, xlab=TeX("$\\Delta IQ"), main=TeX("$\\Delta IQ$ histogram, Oceania"))
hist(data3[,"Fullscale_IQ_Change"], col="blue", border=F, freq=F, xlab=TeX("$\\Delta IQ"), main=TeX("$\\Delta IQ$ histogram, Africa"))
hist(data4[,"Fullscale_IQ_Change"], col="magenta", border=F, freq=F, xlab=TeX("$\\Delta IQ"), main=TeX("$\\Delta IQ$ histogram, Asia"))
```

```{r, data_viz_SI_hist, fig.show="hold", echo=FALSE}
# Schooling Index histogram for continent
hist(data5[,"Schooling_Index"], col="orange",  border=F, freq=F, xlab=TeX("Schooling level"), main=TeX("Schooling level histogram, America"))
hist(data1[,"Schooling_Index"], col="red", border=F, freq=F, xlab=TeX("Schooling level"), main=TeX("Schooling level histogram, Europe"))
hist(data2[,"Schooling_Index"], col="green", border=F, freq=F, xlab=TeX("Schooling level"), main=TeX("Schooling level histogram, Oceania"))
hist(data3[,"Schooling_Index"], col="blue", border=F, freq=F, xlab=TeX("Schooling level"), main=TeX("Schooling level histogram, Africa"))
hist(data4[,"Schooling_Index"], col="magenta", border=F, freq=F, xlab=TeX("Schooling level"), main=TeX("Schooling level histogram, Asia"))
```

# Data description

Our data is a collection of time indexed IQ test score changes for a
given continents (Asia, Europe, Oceania, Africa, America). Successive
data points are the developments in IQ as compared to the first
measurement made, such that at the first Fullscale IQ Change is zero.
Schooling levels are not set to be zero for the first data point but
this didn't seem to cause problems for the models.

The Full-scale IQ data has previously been used in a frequentist
meta-analysis which gathered global historical IQ data from 1909 to 2013
and found global non-linear increases across all IQ domains. Best
explanation for IQ increases was found in increasing nutrition,
education and lower family size. [@IQ_paper].

Our analysis differs in that it is bayesian and studies the relationship
between education level and IQ directly. This approach is vulnerable to
overfitting assuming education level is determined by for example income
level but this is not a concern for us as we are interested in the link
between education level and IQ increase specifically. Schooling level or
time could also both explain both of the models if they are dependent on
each other, as they can be assumed to be. The joint models which take
into account both temporal change and schooling level, were they to
perform better than the separate models, would reveal a d

The education level data has been analysed before with a trend analysis
from 1870 onwards discovering gains globally that do not match global
wealth distribution such that the global middle class has seen the best
gains overall. [@AHDI_paper].

Our analysis is bayesian and concerned with the link to IQ which has not
previously been explored for this data set.

-   Full-scale IQ Data acquired from
    [https://github.com/owid/owid-datasets/tree/master/datasets/IQ%20Data%20-%20Pietschnig%20and%20Voracek%20(2015)](https://github.com/owid/owid-datasets/tree/master/datasets/IQ%20Data%20-%20Pietschnig%20and%20Voracek%20(2015)){.uri}
    (Accessed 2021-11-23), "IQ Data - Pietschnig and Voracek
    (2015).csv", preprocessing discussed in [Appendix A: Preprocessing]
-   Education data acquired from
    <https://frdelpino.es/investigacion/en/category/01_social-sciences/02_world-economy/03_human-development-world-economy/>
    (Accessed 2021-11-23)("AHDI countries 1.1 (xlsx)", table
    "Schooling_Index", preprocessing discussed in [Appendix A:
    Preprocessing])

# Model description

The data is organized into continents so we will model the difference
and similarity in continent behavior with a separate, pooled and
hierarchical model.

The pooled model will pool all results into a global pool for which we
will simulate the following models
\begin{align*}
  &1: \Delta IQ \sim N(\alpha + \beta \cdot \text{decade}, 1)\\
  &2: \Delta IQ \sim N(\alpha + \gamma \cdot \text{schooling level}, 1)\\\
  &3: \Delta IQ \sim N(\alpha + \beta \cdot \text{decade} + \gamma \cdot \text{schooling level}, 1)\\
\end{align*}

Each model will use weakly informative priors such that 
\begin{align*}
  \alpha ~ N(0, 1)\\
  \beta ~ N(0, 50)\\
  \gamma ~ N(0, 100)\\
\end{align*}

These priors are based on our prior beliefs as follows: + The model
should naturally intercept at zero, since it is modelling change to the
initial state. The intercept is normally distributed to better detect
unexpected behavior in the model. The variance is quite strict since the
maximum change in IQ in the data is around 40 points but this should not
matter since the model is not concerned with modelling anything at
$t=0$. + The temporal slope $\beta$, a slope that expresses the
dependence between the monotonically increasing decade attribute and the
change in IQ, should be allowed to, in terms of the prior distribution,
explain all the change in the data and allowing for changes outside of
the data. Our prior beliefs dictate that seeing a 100 point change would
be very very unlikely but still allowed in terms of probabilities.
There's conceptually an over-fitting to the data here but a more
dispersed prior would be very strange rationally i.e. a decade resulting
in a 100 IQ point change given that the maximum IQ score is 200. + The
schooling slope, $\gamma$, that expresses the dependence between IQ
change and schooling level is chosen with the same rationale as the
slope $\beta$ expect scaled to fit the smaller data values (schooling
level values vary between 0 and 1 such that 1 implies university level
education). + $y$'s variance, the noise, is distributed normally and
somewhat strictly due to us not wanting for much of the variance to be
explained by noise, believing that a stricter noise would result in us
detecting bad fits for our models more quickly.

The hierarchical model will have continents sharing variance for the
slopes and intercepts such that for continent $c\in C$ 
\begin{align*}
  \alpha_c ~ N(0, 1)\\
  \beta_c ~ N(0, 50)\\
  \gamma_c ~ N(0, 100)\\
  \sigma ~ N(0, 100)\\
\end{align*} Where $\sigma$ is the shared variance between continents
for the linear combination of attributes. In total, the hierarchical
model is then constructed such that \begin{align*}
  \Delta IQ_c \sim N(\alpha_c + \beta_c \cdot \text{decade} + \gamma_c \cdot \text{schooling level}, \sigma)
\end{align*}

# Stan code

Stan Code ran as follows () how we ran the code (how many chains,
iterations, warmup legnth, example line (fit \<- stan(...)))

Due to its length we have deciced to place the Stan source code in
[Appendix C: Stan source code]

code...

how



```{r echo=FALSE, eval=FALSE}
plot(x=a, y=a)
```

# Convergence diagnostics
\begin{table}[H]
\centering
\begin{tabular}{|c | c | r|r|r|r|r|}
    \hline
    \multicolumn{2}{|c|}{\phantom{Separate simple}} & Europe & Oceania & Africa & Asia & America\\
    \hline
    \multicolumn{7}{|l|}{\textbf{Separate simple}}\\
    \hline
    \multirow{2}{5em}{$\widehat{R}$} & $a$ & 1.00118 & 1.002355 & 1.000308 & 1.003236 & 1.000922 \\ \cline{2-7}
    & $b$ & 1.00118 & 1.002355 & 1.000308 & 1.003236 & 1.000922\\ \hline
    \multirow{2}{5em}{$n_{eff}$} & $a$ & 2463 & 2772 & 3051 & 4591 & 3711 \\ \cline{2-7}
    & $b$ & 2502 & 2863 & 2837 & 3584 & 2740\\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\
    \hline
    \multicolumn{7}{|l|}{\textbf{Separate with Schooling Index}}\\
    \hline
    \multirow{2}{5em}{$\widehat{R}$} & $a$ & 1.00961 & 1.003344 & 1.001653 & 1.000237 & 1.003206 \\ \cline{2-7}
    & $c$ & 1.000961 & 1.003344 & 1.001653 & 1.000237 & 1.003206\\ \hline
    \multirow{2}{5em}{$n_{eff}$} & $a$ & 3866 & 2796 & 2943 & 4826 & 4767 \\ \cline{2-7}
    & $c$ & 3238 & 3414 & 3080 & 2644 & 3006 \\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\
    
    \hline
    \multicolumn{7}{|l|}{\textbf{Separate with Decade and Schooling Index}}\\
    \hline
    \multirow{3}{5em}{$\widehat{R}$} & $a$ & 1.0013204 & 1.0003936 & 1.0010471 & 1.0005874 & 0.996429 \\ \cline{2-7}
    & $b$ & 1.0013204 & 1.0003936 & 1.0010471 & 1.0005874 & 0.996429 \\ \cline{2-7}
    & $c$ & 1.001945 & 1.001503 & 1.001095 & 1.00122 & 1.00092 \\ \hline
    \multirow{3}{5em}{$n_{eff}$} & $a$ & 2491 & 2362 & 2676 & 5330 & 5337 \\ \cline{2-7}
    & $b$ & 2058 & 1989 & 2270 & 1999 & 2199 \\ \cline{2-7}
    & $c$ & 1896 & 1862 & 2107 & 1982 & 2163 \\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\ \hline
\end{tabular}
\caption{Convergence diagnostics for the separate model}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c | c | r|r|r|r|r|}
    \hline
    \multicolumn{2}{|c|}{\phantom{Hierarchical temporal}} & Europe & Oceania & Africa & Asia & America\\
    \hline
    \multicolumn{7}{|l|}{\textbf{Hierarchical temporal}}\\
    \hline
    \multirow{2}{5em}{$\widehat{R}$} & $a$ & 1.000584 & 1 .0012687 & 0.9999413 &1.0000693 & 0.9999622 \\ \cline{2-7}
    & $b$ & 0.9998691 & 1.0000395 & 1.0008906 & 1.0002912 & 1.0005587\\ \hline
    \multirow{2}{5em}{$n_{eff}$} & $a$ & 4468 & 4302 & 6278 & 5025 & 4971 \\ \cline{2-7}
    & $b$ & 4809 & 4902 & 5227 & 4964 & 5097\\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\
    \hline
    \multicolumn{7}{|l|}{\textbf{Hierarchical with Schooling Index}}\\
    \hline
    \multirow{2}{5em}{$\widehat{R}$} & $a$ & 1.000805 & 1.001127 & 1.000538 & 1.000633 & 1.000593 \\ \cline{2-7}
    & $c$ & 1.0001844&1.0010336&1.0005288&1.0001078&0.9999031\\ \hline
    \multirow{2}{5em}{$n_{eff}$} & $a$ & 4085 & 4013 & 4356 & 4096 & 4373 \\ \cline{2-7}
    & $c$ & 3531 & 3748 & 4119 & 3796 & 4230 \\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\
    
    \hline
    \multicolumn{7}{|l|}{\textbf{Hierarchical with Decade and Schooling Index}}\\
    \hline
    \multirow{3}{5em}{$\widehat{R}$} & $a$ & 1.000985 & 1.001348 & 1.002005 & 1.002113 & 1.000064 \\ \cline{2-7}
    & $b$ & 1.0009266 & 0.9996498 & 1.0015432 & 1.0014392 & 1.0006435 \\ \cline{2-7}
    & $c$ & 1.0010106 & 0.9994411 & 1.0020594 & 1.0014008 & 1.0005017 \\ \hline
    \multirow{3}{5em}{$n_{eff}$} & $a$ & 5330 & 5317 & 4493 & 3833 & 5855 \\ \cline{2-7}
    & $b$ & 3490 & 3542 & 2998 & 3012 & 3037 \\ \cline{2-7}
    & $c$ & 3353 & 3421 & 2884 & 2795 & 2970 \\ \hline
    \multicolumn{7}{|l|}{No divergence.}\\ \hline
    \multicolumn{7}{|l|}{Maximum tree depth of 10 never saturated}\\ \hline
\end{tabular}
\caption{Convergence diagnostics for the hierarchical model}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l | r | r|}
    \hline
    \phantom{Pooled temporal} & $\widehat{R}$ & $n_{eff}$\\
    \hline
    \multicolumn{3}{|l|}{\textbf{Pooled temporal}}\\
    \hline
    $a$ & 1.001166 & 2719\\ \hline
    $b$ & 1.001959 & 2596\\ \hline
    \multicolumn{3}{|l|}{No divergence.}\\ \hline
    \multicolumn{3}{|l|}{Maximum tree depth of 10 never saturated}\\
    \hline
    \multicolumn{3}{|l|}{\textbf{Pooled with Schooling Index}}\\
    \hline
    $a$ & 1.000503 & 2858\\ \hline
    $b$ & 1.001412 & 2603\\ \hline
    \multicolumn{3}{|l|}{No divergence.}\\ \hline
    \multicolumn{3}{|l|}{Maximum tree depth of 10 never saturated}\\
    \hline
    \multicolumn{3}{|l|}{\textbf{Pooled with Decade and Schooling Index}}\\
    \hline
    $a$ & 1.001149 & 2994\\ \hline
    $b$ & 1.001361 & 2195\\ \hline
    $c$ & 1.000848 & 2057\\ \hline
    \multicolumn{3}{|l|}{No divergence.}\\ \hline
    \multicolumn{3}{|l|}{Maximum tree depth of 10 never saturated}\\
    \hline
    
\end{tabular}
\caption{Convergence diagnostics for the pooled model}
\end{table}

$\mathbf{\widehat{R}}$ is roughly the ratio of the parameter variance when the data is
pooled across all of the chains to the within-chain variance. It
basically measures how much the chains are reaching different
conclusions. Since most of the Rhats above are very close to 1, and all
of them are less than 1.1, we can conclude that the chains reach the
same conclusion.

$\mathbf{n_{eff}}$ is an estimate of the effective sample size of the parameters.
The smaller it is, the greater is the uncertainty associated with its
parameter. A common heuristic is that the nEff needs to be higher than
100 for parameters of interest. Since this is the case with the above
models, we can be sure that there is low uncertainty associated with the
model parameters.

**Divergence** implies that the drawn samples are not from the entire
posterior and thus the inferences will be biased. Since the above models
do not diverge, we can conclude that they sample from the entire
posterior.

Crossing the maximum **tree depth** implies that the optimal number of
steps for each iteration is higher than the currently set maximum. Since
this is not the case in the above models, we can conclude that the
sampler is efficient and the optimal number of steps for each iteration
is less than the maximum tree depth.



# Performance assessment

Plotted values for deltaIQ

PSIS-LOO?


# Posterior predictive checks

Visual posterior predictive checking was performed for the three pooled models due to them being the most interesting due to their best performance.

Posterior predictive checking for the Full-scale IQ Change ($\Delta IQ$) was done by sampling 100 distributions (of 54 predicted observation) from the 4000 generated predictions. The resulting plots are below.
In the plots the red line is the distribution of the original sample with the semi-transparent
black lines being generated predictions.

```{r posterior_predictive_checking, echo=FALSE}
# ppc for each variant of the pooled model

# ppc for decade pooled
ypred_temp <-extract(fit_pool_temp, pars = c("y_pred"))$`y_pred`
#ypred_temp[60,]
ypred_school <-extract(fit_pool_school, pars = c("y_pred"))$`y_pred`
ypred_comb <-extract(fit_pool_comb, pars = c("y_pred"))$`y_pred`

plot(density(x2), ylim=c(0, 0.058), xlim=c(-30, 60), col="red",
     main="Posterior predictive densities, temporal model", xlab=TeX("$\\Delta IQ$"))

for(i in seq(1, 4000, 100)) {
  lines(density(ypred_temp[i,]), col=alpha("black", 0.3), lty=1)
}
lines(density(x2), col="red")

# ppc for school index model
plot(density(x2), ylim=c(0, 0.058), xlim=c(-30, 60), col="red",
     main="Posterior predictive densities, Schooling Index", xlab=TeX("$\\Delta IQ$"))
for(i in seq(1, 4000, 100)) {
  lines(density(ypred_school[i,]), col=alpha("black", 0.3), lty=1)
}
lines(density(x2), col="red")

# ppc for combined model
plot(density(x2), ylim=c(0, 0.058), xlim=c(-30, 60), col="red",
     main="Posterior predictive densities, Combined model", xlab=TeX("$\\Delta IQ$"))
for(i in seq(1, 4000, 100)) {
  lines(density(ypred_comb[i,]), col=alpha("black", 0.3), lty=1)
}
lines(density(x2), col="red")
```


The plots show that the predictions roughly follow the original distribution pretty well for the most part but with a lot of uncertainty towards the middle. These results seem good to us taking into account the size of the data.

# Predictions

```{r, eval=TRUE}
#ans <- extract(fit_sep_temp)
#exp_val1 <- mean(ans$ypred)
#q1 <- quantile(ans$ypred, c(0.05, 0.95))

#ans <- extract(fit_sep_comb)
#exp_val_21 <- mean(ans$ypred[,1])
#exp_val_22 <- mean(ans$ypred[,2])
#exp_val_23 <- mean(ans$ypred[,3])
#q21 <- quantile(ans$ypred[,1], c(0.05, 0.95))
#q22 <- quantile(ans$ypred[,2], c(0.05, 0.95))
#q23 <- quantile(ans$ypred[,3], c(0.05, 0.95))

#samples
exp_val1 <- 1
q1 = c(0.99, 1.01)
exp_val_21 <- 1
exp_val_22 <- 2
exp_val_23 <- 3
q21 <- c(0.99, 1.01)
q22 <- c(2, 2.02)
q23 <- c(3, 3.01)
```

We computed predictions for IQ increases in subsequent decades for the pooled models (with only the decade index, and the model with both the decade index and the schooling index), since the pooled model infact had the best fit to the sample data. In the data, the latest observations were 10 decades from the initial starting point (decade index = 0). Therefore we predicted the IQ change 11 decades after the start point. For the simple (single predictor) pooled model, the estimate for the expected value of predicted IQ change was $`r round(exp_val1,2)`$ while the 90% credible interval for the same was $[ `r round(q1[1],2)` , `r round(q1[2],2)`]$.

The prediction estimates for the other model required more involved analyses. This is because we had to come up with reliable values of the schooling index to put into the model for prediction. Since the pooled model considers all the continents interchangeable, there was no clear way to find the value of the schooling index 11 decades after the starting point. Thus we chose three sample values which were 5%, 10%, and 15% more than the latest mean value of the AHDI index. Thus we obtained three different estimates, based on the three possible (and feasible) values of the schooling index. Based on this the estimates were $`r round(exp_val_21,2)`$, $`r round(exp_val_22,2)`$, and $`r round(exp_val_23,2)`$ with 90% credible intervals $[ `r round(q21[1],2)`, `r round(q21[2],2)`]$, $[`r round(q22[1],2)`,`r round(q22[2],2)`]$, and $[`r round(q23[1],2)`,`r round(q23[2],2)`]$ respectively.

# Sensitivity Analysis

Prior sensitivity analysis will be done on the simple and advanced
pooled model because they are the most interesting due to their best performance.
Changing priors didn't have an effect on the relative performance of the pooled, separate and hierarchical models.

Change intercept $a$ prior from $N(0,1)$ to $N(0, 10)$

Reason: Even though the data starts from the 0-point, it is not
necessary that the best fit line should also start there. The modified
prior offers the y-intercept a bit more freedom in these regards.

Simple Model

```{r, include=FALSE}
IQ <- list(N = length(x3),
           x1 = x4,
           x2 = x3,
           y = x2,
           xpred = 11
)
fit_default <- stan(file = 'models/pooled/pool.stan', data = IQ, seed=1)
ans_default <- extract(fit_default)

fit_prior1 <- stan(file = 'models/pooled/pool_prior1.stan', data = IQ, seed=1)
ans_prior1 <- extract(fit_prior1)
```

```{r, echo=FALSE}
hist( ans_default$a, col="lightblue", xlab="a", main="Histogram of a with different priors for simple pooled model")  # first histogram
hist( ans_prior1$a, col=rgb(1,0,0,1/4), add=T)  # second
legend(x="topright", legend=c("Normal Prior", "Modified Prior"), fill=c("lightblue", rgb(1,0,0,1/4)))

```

Advanced Model

```{r, include=FALSE}
IQ <- list(N = length(x3),
           x1 = x4,
           x2 = x3,
           y = x2,
           xpred_decade = 11,
           xpred_schooling = mu_vec
)
fit_default <- stan(file = 'models/pooled/pool_adv.stan', data = IQ, seed=1)
ans_default <- extract(fit_default)

fit_prior1 <- stan(file = 'models/pooled/pool_adv_prior1.stan', data = IQ, seed=1)
ans_prior1 <- extract(fit_prior1)
```

```{r, echo=FALSE}
hist( ans_default$a, col="lightblue", xlab="a", "Histogram of a with different priors for advanced pooled model")  # first histogram
hist( ans_prior1$a, col=rgb(1,0,0,1/4), add=T)  # second
legend(x="topright", legend=c("Normal Prior", "Modified Prior"), fill=c("lightblue", rgb(1,0,0,1/4)))

```

Analysis: $a$'s distribution, when the prior is changed from $N(0,1)$ to
$N(0,10)$ remains a normal distribution but shifts its center from
$0.86$ to $2.4$. This implies that our model is sensitive to the prior
distribution of $a$.

Change $\sigma$ prior from $N(0, 100)$ to $Inv-\chi^2(1)$

Reason: Inverse-chi squared is generally used as the prior for an
unknown variance of the normal distribution. This is so because it is a
conjugate prior, which for example makes the computations easier, and it
also satisfies the minimal requirements needed for a prior variance.

Simple Model

```{r, include=FALSE}
IQ <- list(N = length(x3),
           x1 = x4,
           x2 = x3,
           y = x2,
           xpred = 11
)
fit_default <- stan(file = 'models/pooled/pool.stan', data = IQ, seed=1)
ans_default <- extract(fit_default)

fit_prior2 <- stan(file = 'models/pooled/pool_prior2.stan', data = IQ, seed=1)
ans_prior2 <- extract(fit_prior1)
```

```{r, echo=FALSE}
hist( ans_default$sigma, col="lightblue", xlab="sigma", main="Histogram of sigma with different priors for simple pooled model")  # first histogram
hist( ans_prior2$sigma, col=rgb(1,0,0,1/4), add=T)  # second
legend(x="topright", legend=c("Normal Prior", "Modified Prior"), fill=c("lightblue", rgb(1,0,0,1/4)))
```

Advanced Model

```{r, include=FALSE}
IQ <- list(N = length(x3),
           x1 = x4,
           x2 = x3,
           y = x2,
           xpred_decade = 11,
           xpred_schooling = mu_vec
)
fit_default <- stan(file = 'models/pooled/pool_adv.stan', data = IQ, seed=1)
ans_default <- extract(fit_default)

fit_prior2 <- stan(file = 'models/pooled/pool_adv_prior2.stan', data = IQ, seed=1)
ans_prior2 <- extract(fit_prior1)
```

```{r, echo=FALSE}
hist( ans_default$sigma, col="lightblue", xlab="sigma", main="Histogram of sigma with different priors for advanced pooled model")  # first histogram
hist( ans_prior2$sigma, col=rgb(1,0,0,1/4), add=T)  # second
legend(x="topright", legend=c("Normal Prior", "Modified Prior"), fill=c("lightblue", rgb(1,0,0,1/4)))
```

Analysis: $\sigma$'s distribution stays almost identical when changed
from $N(0,100)$ to $inv-\chi^2(1)$. This implies that our model is
insensitive to the prior distribution of sigma.


# Issues and improvements

Close to the end of the project we noticed that our data had been
constructed in a way that is conceptually difficult for linear
regression. Mainly the same explanatory variable value would lead to
different response variable values. This does not cause problems for the
linear regression algorithms used but makes understanding the data more
difficult. In essence a concrete uncertainty in the data. Changing this
however didn't alter our results in terms of model selection or relative
PSIS-LOOs so we decided to leave it as is.



# Conclusion

In conclusion, we managed to find the Flynn effect in the data and find a well performing model
linking schooling level to cumulative Full-scale IQ change over time. This happened
to be the pooled model in our case. The pooled model performing significantly
better than the other models implies that this phenomenon is global such that measurements
from different continents are interchangeable. Our best performing model also predicted
that the observed increase would continue with a XXX point increase in the next decade.

# Self-reflection

We also should have consulted the TAs more but, even though we started
early, we didn't have time to do this because the project started to
concretize later than we expected due to our slow progress. This meant
that at the point at which questions arose we no longer had time to ask
them. This was due to a failure in planning and overestimating our pace.
From this we learned to be better planners.

# References

::: {#refs}
:::

# Appendix

## Appendix A: Preprocessing

All data preprocessing was done in Python using standard Data Science
libraries such as Numpy and Pandas with the help of SQL to combine
datasets. The general workflow included first preprocessing the
"Fullscale_IQ_Change" dataset, followed by preprocessing the
"AHDI_countries" dataset. Next, these datasets were combined, and some
minor transformations were made to make the data suit our model better.
Finally it was converted to a csv file which used by all the Stan
models.

Preprocessing the "Fullscale_IQ_Change" dataset was fairly easy, since
it did not have any null values. Our first data transformation operation
was to convert the year column to a decade column. This transformation
required a trivial integer division by 10 for all the years and then
multiplying them with 10. On the other hand, the "AHDI_countries"
dataset required much more work. Firstly, we had to determine which
continent each country belonged to, which had to be done mostly
manually. Here, all the data was already listed according to decade,
which reduced some preprocessing steps. Once all the countries had been
classified, we replaced the countries with their continents and took the
mean of the AHDI over all continents for every decade in the dataset.

Now, that both the datasets were cleaned, we needed to combine them. We
determined that SQL would be the best tool for this operation. However,
since we were using Python, we decided to just use the PandaSQL library
which allowed us to use SQL with Pandas dataframes. This library allowed
to join both the datasets with simple join query. The final change we
made to the combined dataset was to change the decade column to decade
index. The decade index set the first decade observation of each
continent to be 0, and every subsequent decade to be incremented by 1.
For example, the first decade observation for Europe was 1910 which
implied the decade index 0, and the following observations from 1930,
1950, and 1960 were given decade indices 2, 4, 5 respectively. Lastly,
we converted this dataframe into a csv file.

## Appendix B: R source code

!!!!!!!!!!!!!!!!!COPY ALL IMPORTANT CODEZ HERE

## Appendix C: Stan source code
